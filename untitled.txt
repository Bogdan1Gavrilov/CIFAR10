import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from src.CosineWarmupSCH import CosineAnnWarmupRestart
from src.mixup import mixup_data
from src.EMAmodel import EMA
from adan_pytorch import Adan

from src.prepare_data import get_data_loaders
from models.ResModelV2 import ResNetV2


#1. Настраиваем устройство и гиперпараметры
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
epochs = 50
lr = 0.001
batch_size = 64

#2.Даталоадер
train_loader, test_loader = get_data_loaders(batch_size=batch_size)

#3.Функция потерь, модель, оптимизатор, EMA-model

model = ResNetV2().to(device)
ema = EMA(model, decay=0.998)
criterion = nn.CrossEntropyLoss(label_smoothing=0.1) #Добавил 10% сглаживание меток
optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=1e-6)
steps_per_epoch = len(train_loader)
scheduler = CosineAnnWarmupRestart(
    optimizer,
    first_cycle_steps=10 * len(train_loader),  # 10 эпох первый цикл
    cycle_mult=2,#Коэфф возрастания длины цикла
    max_lr=0.001,#максимальный lr
    min_lr=1e-6,#минимальный lr к которому стремится модель
    warmup_steps=2 * len(train_loader),       # 2 эпохи разогрева
    gamma=0.9#Коэффициент затухания max_lr
)


#4. Логгер
writer = SummaryWriter(log_dir="runs/cifar10_exp_22")

#5. Цикл обучения
for epoch in range(epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        # Mixup аугментация
        images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=0.4)

        #Обнуляем градиенты
        optimizer.zero_grad()

        #Прямой проход
        outputs = model(images.to(device))
        if torch.isnan(outputs).any():
            print("NaN в output модели!")
            break
        #Потери и градиенты
        # Используем модифицированную функцию потерь для миксапа:
        loss = lam * criterion(outputs, targets_a) + (1 - lam) * criterion(outputs, targets_b)
        if torch.isnan(loss):
            print("NaN в лоссе!")
            break
        loss.backward()
        optimizer.step()
        scheduler.step()
        ema.update()
        
        #Статистика
        running_loss += loss.item()
        _, predicted = outputs.max(1)
        total += labels.size(0)
        correct += predicted.eq(labels).sum().item()

    epoch_loss = running_loss / len(train_loader)
    epoch_acc = 100. * correct/total

    print(f"Epoch [{epoch+1}/{epochs}] Loss: {epoch_loss:.4f} | Accuracy: {epoch_acc:.2f}%")
    writer.add_scalar("Loss/train", epoch_loss, epoch)
    writer.add_scalar("Accuracy/train", epoch_acc, epoch)
    for param_group in optimizer.param_groups:
        current_lr = param_group['lr']
        writer.add_scalar("LearningRate", current_lr, epoch)


#6.Сохранение весов модели
print("Применяем EMA веса для финального сохранения...")
ema.apply_shadow()
os.makedirs("weights", exist_ok=True)
torch.save(model.state_dict(), "weights/cifar10_res50netV2-.pth")

writer.close()